{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"genesis_ros","text":"<p>Train your ROS 2 robot inside genesis and deploy it to real!</p> <p>If you want to know</p> <ul> <li>How to setup genesis_ros? =&gt; Setup</li> <li>How to train robot? =&gt; PPO</li> </ul>"},{"location":"setup/","title":"How to setup","text":""},{"location":"setup/#install-uv","title":"Install UV","text":"<p>Please follow UV official documentation.</p>"},{"location":"setup/#setup-genesis_ros","title":"Setup genesis_ros","text":"<p>Clone source code from genesis_ros github repository.</p> <pre><code>git clone https://github.com/team-re-boot/genesis_ros.git\ncd genesis_ros\nuv sync\n</code></pre>"},{"location":"algorithms/ppo/","title":"PPO","text":"<p>Proximal Policy Optimization</p> <p>This package integrate rsl_rl package and use PPO algorithm inside this library.</p>"},{"location":"algorithms/ppo/#how-to-make-your-experiment","title":"How to make your experiment.","text":"<p>Example source code is in this directory.</p> <pre><code>.\n\u2514\u2500\u2500 go2_walking # Name of the experiment\n    \u251c\u2500\u2500 command_config.yaml # Configuration for the robot command\n    \u251c\u2500\u2500 entities.py # Python script for listing up entities inside experiment\n    \u251c\u2500\u2500 environment_config.yaml # Configuration for the environment\n    \u251c\u2500\u2500 observation_config.yaml # Configuration for the observation\n    \u251c\u2500\u2500 reward_functions.py # Python script for define reward functions\n    \u251c\u2500\u2500 simulation_config.yaml # Configuration for the simulation\n    \u2514\u2500\u2500 train_config.yaml # Configuration for the training\n\n1 directory, 7 files\n</code></pre>"},{"location":"algorithms/ppo/#command_configyaml","title":"command_config.yaml","text":"<p>Note</p> <p>This yaml file is optional for the experiment.</p> <pre><code>num_commands: 3 # number of commands\nlin_vel_x_range: [0.5, 0.5] # range of linear velocity in x direction\nlin_vel_y_range: [0.0, 0.0] # range of linear velocity in y direction\nang_vel_range: [0.0, 0.0] # range of angular velocity\n</code></pre> <p>This file describes the configuration for the robot command.</p> <p>In this experiment, the robot can be given a speed command.</p> <p>The speed commands are given in the forward and backward directions and in the direction of rotation.</p>"},{"location":"algorithms/ppo/#entitiespy","title":"entities.py","text":"<p>Warning</p> <p>This python script is required for the experiment.</p> <pre><code>import genesis as gs\nfrom typing import List\n\n\ndef get_entities() -&gt; List[gs.morphs.Morph]:\n    return [gs.morphs.Plane()]\n</code></pre> <p>This python script describes the configuration for the entities inside simulation.</p> <p>This script must contain function named <code>get_entities()</code> with <code>List[gs.morphs.Morph]</code> return type.</p>"},{"location":"algorithms/ppo/#environment_configyaml","title":"environment_config.yaml","text":"<p>Note</p> <p>This yaml file is optional for the experiment.</p> <pre><code>default_joint_angles: # The default joint angles for the robot\n  FL_hip_joint: 0.0 # Front left hip joint, this name comes from the robot URDF\n  FR_hip_joint: 0.0\n  RL_hip_joint: 0.0\n  RR_hip_joint: 0.0\n  FL_thigh_joint: 0.8\n  FR_thigh_joint: 0.8\n  RL_thigh_joint: 1.0\n  RR_thigh_joint: 1.0\n  FL_calf_joint: -1.5\n  FR_calf_joint: -1.5\n  RL_calf_joint: -1.5\n  RR_calf_joint: -1.5\nkp: 20.0 # Proportional gain for the PD controller\nkd: 0.5 # Derivative gain for the PD controller\nbase_init_pos: [0.0, 0.0, 0.42] # Initial position of the robot base\nbase_init_quat: [1.0, 0.0, 0.0, 0.0] # Initial orientation of the robot base\nepisode_length_seconds: 20.0 # Length of the episode in seconds\nresampling_time_seconds: 4.0 # Time between resampling the action\naction_scale: 0.25 # Scale for the action space\nsimulate_action_latency: true # Whether to simulate action latency\nclip_action: 100.0 # Clip the action to a certain range\n</code></pre> <p>This file describes the configuration for the simulation environment.</p> <p>The initial posture of the robot, hyperparameter of the PPO algorithm, etc. can be set.</p>"},{"location":"algorithms/ppo/#observation_configyaml","title":"observation_config.yaml","text":"<p>Note</p> <p>This yaml file is optional for the experiment.</p> <pre><code>obs_scales: # Scale for each observation\n  lin_vel: 2.0 # Scale for linear velocity\n  ang_vel: 0.25 # Scale for angular velocity\n  dof_pos: 1.0 # Scale for joint position\n  dof_vel: 0.05 # Scale for joint velocity\n</code></pre> <p>This file describes the configuration for the observation.</p> <p>Currently, only the Observation scale can be set.</p>"},{"location":"algorithms/ppo/#reward_functionspy","title":"reward_functions.py","text":"<p>Warning</p> <p>This python script is required for the experiment.</p> <pre><code>import torch\n\n\ndef get_reward_functions():\n    reward_functions = []\n\n    # ------------ reward functions----------------\n    def reward_tracking_lin_vel(self):\n        # Tracking of linear velocity commands (xy axes)\n        lin_vel_error = torch.sum(\n            torch.square(self.commands[:, :2] - self.base_lin_vel[:, :2]), dim=1\n        )\n        return torch.exp(-lin_vel_error / 0.25)\n\n    reward_functions.append((reward_tracking_lin_vel, 1.0))\n\n    def reward_tracking_ang_vel(self):\n        # Tracking of angular velocity commands (yaw)\n        ang_vel_error = torch.square(self.commands[:, 2] - self.base_ang_vel[:, 2])\n        return torch.exp(-ang_vel_error / 0.25)\n\n    reward_functions.append((reward_tracking_ang_vel, 0.2))\n\n    def reward_lin_vel_z(self):\n        # Penalize z axis base linear velocity\n        return torch.square(self.base_lin_vel[:, 2])\n\n    reward_functions.append((reward_lin_vel_z, -1.0))\n\n    def reward_action_rate(self):\n        # Penalize changes in actions\n        return torch.sum(torch.square(self.last_actions - self.actions), dim=1)\n\n    reward_functions.append((reward_action_rate, -0.005))\n\n    def reward_similar_to_default(self):\n        # Penalize joint poses far away from default pose\n        return torch.sum(torch.abs(self.dof_pos - self.default_dof_pos), dim=1)\n\n    reward_functions.append((reward_similar_to_default, -0.1))\n\n    def reward_base_height(self):\n        # Penalize base height away from target\n        return torch.square(self.base_pos[:, 2] - 0.3)\n\n    reward_functions.append((reward_base_height, -50.0))\n\n    return reward_functions\n</code></pre> <p>This python script defines the reward functions in this experiment.</p> <p>This python script must contain <code>get_reward_functions()</code> function with the return value is a function object that takes self as its first argument and changes torch.Tensor to a list of tuples of type float.</p> <p>1st element of the tuple means the each reward function, 2nd element of the tuple means the scale of the each reward function.</p> <p>The functions defined in this script are added as member functions of the PPOEnv class defined in ppo_env.py and executed at each simulation frame.</p>"},{"location":"algorithms/ppo/#simulation_configyaml","title":"simulation_config.yaml","text":"<p>Note</p> <p>This yaml file is optional for the experiment.</p> <pre><code>simulate_action_latency: True # Whether to simulate action latency\ndt: 0.02  # Time step for the simulation\n</code></pre> <p>This file describes the configuration for the simulation latency and time step.</p>"},{"location":"algorithms/ppo/#train_configyaml","title":"train_config.yaml","text":"<p>Note</p> <p>This yaml file is optional for the experiment.</p> <pre><code>algorithm: ppo # Algorithm to use\n\npolicy: # Settings for the policy. See also, https://github.com/leggedrobotics/rsl_rl\n  activation: elu # Activation function for the policy network\n  actor_hidden_dims: [512, 256, 128] # Hidden dimensions for the actor network\n  critic_hidden_dims: [512, 256, 128] # Hidden dimensions for the critic network\n  init_noise_std: 1.0 # Initial noise standard deviation\n  class_name: ActorCritic # Loading the ActorCritic class\n\nrunner:\n  experiment_name: go2_walking # Name of the experiment\n  checkpoint: -1 # Checkpoint to load, -1 means the latest checkpoint\n  load_run: -1 # Load run number, -1 means the latest run\n  log_interval: 1 # Interval for logging\n  max_iterations: 101 # Maximum number of iterations\n\nrunner_class_name: OnPolicyRunner # Class name for the runner. See also, https://github.com/leggedrobotics/rsl_rl\n</code></pre> <p>This file describes the configuration for the training.</p> <p>This file only needs while training.</p>"},{"location":"algorithms/ppo/#train","title":"Train","text":"<pre><code>uv run ppo_train --config genesis_ros/ppo/config/go2_walking/ --device gpu\n</code></pre> <p>Command usage is below.</p> <pre><code>uv run ppo_train --help\n\nusage: ppo_train [-h] -c CONFIG -d {cpu,gpu} [--num_environments NUM_ENVIRONMENTS] [--urdf_path URDF_PATH]\n\noptions:\n  -h, --help            show this help message and exit\n  -c CONFIG, --config CONFIG\n                        Path to the config directory (default: /home/masaya/workspace/genesis_ros/genesis_ros/ppo/ppo_train.py)\n  -d {cpu,gpu}, --device {cpu,gpu}\n                        Specify device which you want to run PPO and simulation. (default: None)\n  --num_environments NUM_ENVIRONMENTS\n                        Number of environments (default: 4096)\n  --urdf_path URDF_PATH\n                        Path to the URDF file (default: urdf/go2/urdf/go2.urdf)\n</code></pre> <p>If the training script succeed, show dialogs like below.</p> <pre><code>uv run ppo_train --config genesis_ros/ppo/config/go2_walking/ --device gpu\n\nNumber of joints:  12\nJoints :  ['FL_hip_joint', 'FR_hip_joint', 'RL_hip_joint', 'RR_hip_joint', 'FL_thigh_joint', 'FR_thigh_joint', 'RL_thigh_joint', 'RR_thigh_joint', 'FL_calf_joint', 'FR_calf_joint', 'RL_calf_joint', 'RR_calf_joint']\nNumber of actions:  12\nAdding reward function:  reward_tracking_lin_vel\nReward_scale =  1.0\nReward scale considering time delta =  0.02\nAdding reward function:  reward_tracking_ang_vel\nReward_scale =  0.2\nReward scale considering time delta =  0.004\nAdding reward function:  reward_lin_vel_z\nReward_scale =  -1.0\nReward scale considering time delta =  -0.02\nAdding reward function:  reward_action_rate\nReward_scale =  -0.005\nReward scale considering time delta =  -0.0001\nAdding reward function:  reward_similar_to_default\nReward_scale =  -0.1\nReward scale considering time delta =  -0.002\nAdding reward function:  reward_base_height\nReward_scale =  -50.0\nReward scale considering time delta =  -1.0\nReward functions setup finished.\n\nActor MLP: Sequential(\n    (0): Linear(in_features=45, out_features=512, bias=True)\n    (1): ELU(alpha=1.0)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): ELU(alpha=1.0)\n    (4): Linear(in_features=256, out_features=128, bias=True)\n    (5): ELU(alpha=1.0)\n    (6): Linear(in_features=128, out_features=12, bias=True)\n)\nCritic MLP: Sequential(\n    (0): Linear(in_features=45, out_features=512, bias=True)\n    (1): ELU(alpha=1.0)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): ELU(alpha=1.0)\n    (4): Linear(in_features=256, out_features=128, bias=True)\n    (5): ELU(alpha=1.0)\n    (6): Linear(in_features=128, out_features=1, bias=True)\n)\n\n################################################################################\n                        Learning iteration 0/101\n\n                    Computation: 83209 steps/s (collection: 0.861s, learning 0.320s)\n            Value function loss: 0.0125\n                    Surrogate loss: -0.0004\n            Mean action noise std: 1.00\n                Mean total reward: 0.17\n            Mean episode length: 22.88\nMean episode rew_reward_tracking_lin_vel: 0.0107\nMean episode rew_reward_tracking_ang_vel: 0.0020\nMean episode rew_reward_lin_vel_z: -0.0040\nMean episode rew_reward_action_rate: -0.0014\nMean episode rew_reward_similar_to_default: -0.0012\nMean episode rew_reward_base_height: -0.0025\n--------------------------------------------------------------------------------\n                Total timesteps: 98304\n                    Iteration time: 1.18s\n                        Total time: 1.18s\n                            ETA: 119.3s\n\nStoring git diff for 'genesis_ros' in: logs/go2_walking/git/genesis_ros.diff\n################################################################################\n                        Learning iteration 1/101\n\n                    Computation: 189431 steps/s (collection: 0.381s, learning 0.138s)\n            Value function loss: 0.0056\n                    Surrogate loss: -0.0047\n            Mean action noise std: 1.00\n                Mean total reward: 0.39\n            Mean episode length: 44.82\nMean episode rew_reward_tracking_lin_vel: 0.0284\nMean episode rew_reward_tracking_ang_vel: 0.0050\nMean episode rew_reward_lin_vel_z: -0.0053\nMean episode rew_reward_action_rate: -0.0040\nMean episode rew_reward_similar_to_default: -0.0053\nMean episode rew_reward_base_height: -0.0034\n--------------------------------------------------------------------------------\n                Total timesteps: 196608\n                    Iteration time: 0.52s\n                        Total time: 1.70s\n                            ETA: 85.0s\n\n################################################################################\n                    Learning iteration 100/101\n\n                    Computation: 196723 steps/s (collection: 0.359s, learning 0.141s)\n            Value function loss: 0.0003\n                    Surrogate loss: -0.0006\n            Mean action noise std: 0.60\n                Mean total reward: 17.02\n            Mean episode length: 998.39\nMean episode rew_reward_tracking_lin_vel: 0.9393\nMean episode rew_reward_tracking_ang_vel: 0.1766\nMean episode rew_reward_lin_vel_z: -0.0152\nMean episode rew_reward_action_rate: -0.0753\nMean episode rew_reward_similar_to_default: -0.1669\nMean episode rew_reward_base_height: -0.0079\n--------------------------------------------------------------------------------\n                Total timesteps: 9928704\n                    Iteration time: 0.50s\n                        Total time: 50.02s\n                            ETA: 0.5s\n</code></pre>"},{"location":"algorithms/ppo/#train-artifacts","title":"Train artifacts","text":"<p>The artifacts of the training result was output under log directory.</p> <pre><code>logs/\n\u2514\u2500\u2500 go2_walking # Name of the experiment\n    \u251c\u2500\u2500 actor.pt # Torchscript of the actor network.\n    \u251c\u2500\u2500 cfgs.pkl # Pickle file for the configuration, which is necessary fr the model.\n    \u251c\u2500\u2500 events.out.tfevents.1747750787.masaya-System-Product-Name.66931.0 # Log file for tensorflow. Name of the file depends on your environment.\n    \u251c\u2500\u2500 git\n    \u2502   \u2514\u2500\u2500 genesis_ros.diff\n    \u251c\u2500\u2500 model_0.pt # Model of the 0 iteration.\n    \u2514\u2500\u2500 model_100.pt # Model of the 100 iteration.\n\n2 directories, 6 files\n</code></pre> <p>If you want to check training log, run commands below.</p> <pre><code>uv run tensorboard --logdir logs\n</code></pre> <p></p>"},{"location":"algorithms/ppo/#evaluation","title":"Evaluation","text":"<pre><code>uv run ppo_eval -e go2_walking -d gpu --ckpt 100\n</code></pre>"}]}